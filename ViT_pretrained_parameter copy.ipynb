{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "\n",
    "config = resolve_data_config({}, model=model)\n",
    "transform = create_transform(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 224, 224])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc = model.named_children()\n",
    "dc = list(dc)\n",
    "x = torch.arange(4*3*224*224, dtype=torch.float32).reshape(4, 3, 224, 224)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2304, bias=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 0\n",
    "block = dict(dict(dict(model.named_children())['blocks'].named_children())[str(layer)].named_children())\n",
    "# list(block['attn'].children())[0]\n",
    "ls = [list(block['attn'].children())[0], list(block['attn'].children())[4],list(block['mlp'].children())[0], list(block['mlp'].children())[4]]\n",
    "ls[0]\n",
    "# .weight.clone().detach().numpy().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## block binaryfile 생성기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(12):\n",
    "    block = dict(dict(dict(model.named_children())['blocks'].named_children())[str(layer)].named_children())\n",
    "    ls = [block['norm1'],list(block['attn'].children())[0], list(block['attn'].children())[4],block['norm2'],list(block['mlp'].children())[0], list(block['mlp'].children())[4]]\n",
    "\n",
    "    weight_ls = list()\n",
    "    for i in range(len(ls)):\n",
    "        weight_ls.append(ls[i].weight.clone().detach().numpy().T)\n",
    "        weight_ls.append(ls[i].bias.clone().detach().numpy())\n",
    "\n",
    "    blk_weight = np.array(weight_ls[0].flatten())\n",
    "\n",
    "    for i in range(1, len(weight_ls)):\n",
    "        blk_weight = np.concatenate((blk_weight, weight_ls[i].flatten()))\n",
    "    blk_weight.tofile('./pre_weights/'+str(layer)+'_newblock'+'.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('patch_embed', PatchEmbed(\n",
      "  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (norm): Identity()\n",
      "))\n",
      "weight :  torch.Size([768, 3, 256])\n",
      "\n",
      "x:  torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "print(dc[0])\n",
    "print(\"weight : \",dict(dc[0][1].named_children())['proj'].weight.reshape(768, 3, -1).shape)\n",
    "print(\"\\nx: \",x.shape)\n",
    "print(dc[0][1](x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QKV WEIGHT BINFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0\n",
    "dd = dict(list(dc[4][1].children())[layer].named_children())\n",
    "blk = dict(dd['attn'].named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2055, -1.8644, -1.5705,  ...,  0.5286,  0.3821, -0.2423],\n",
       "        [ 1.3894, -0.5281, -1.4717,  ..., -0.2188, -0.8244, -0.4229],\n",
       "        [ 0.8834, -0.5625, -0.1614,  ..., -0.5791, -0.1525, -0.2568],\n",
       "        ...,\n",
       "        [ 0.3980, -1.7263, -1.2302,  ..., -0.6791, -0.0356,  0.7446],\n",
       "        [-0.2545, -2.0227, -1.4990,  ...,  0.3886,  0.2097, -0.2962],\n",
       "        [ 1.8092, -0.4103, -0.1971,  ...,  0.2622,  0.4505,  0.0511]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "dummy_input = torch.randn((4, 196, 768))\n",
    "qkv_w = dict(dd['attn'].named_children())['qkv'].weight.clone().detach().T\n",
    "\n",
    "\n",
    "blk['qkv'](dummy_input)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input.numpy().tofile('./pre_weights/dummy_input_4_196_768.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 196, 768])\n",
      "torch.Size([4, 196, 2304])\n",
      "Identity() torch.Size([4, 196, 2304])\n",
      "Identity() torch.Size([4, 196, 2304])\n",
      "Dropout(p=0.0, inplace=False) torch.Size([4, 196, 2304])\n",
      "Linear(in_features=768, out_features=768, bias=True) torch.Size([4, 196, 2304])\n",
      "<bound method Attention.forward of Attention(\n",
      "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (q_norm): Identity()\n",
      "  (k_norm): Identity()\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method VisionTransformer.forward of VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_ = list(dd.values())\n",
    "x = torch.arange(4 * 196 *768, dtype=torch.float32).reshape(4, 196, 768)\n",
    "x = dd_[0](x)\n",
    "qkv = list(dd_[1].children())\n",
    "print(x.shape)\n",
    "x = qkv[0](x)\n",
    "print(x.shape)\n",
    "x = qkv[1](x)\n",
    "print(qkv[1],x.shape)\n",
    "x = qkv[2](x)\n",
    "print(qkv[2],x.shape)\n",
    "x = qkv[3](x)\n",
    "print(qkv[3],x.shape)\n",
    "# x = qkv[4](x)\n",
    "print(qkv[4],x.shape)\n",
    "print(dd_[1].forward)\n",
    "model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2247, -1.2247, -1.2247],\n",
       "         [-1.2247, -1.2247, -1.2247],\n",
       "         [-1.2247, -1.2247, -1.2247]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 1.2247,  1.2247,  1.2247],\n",
       "         [ 1.2247,  1.2247,  1.2247],\n",
       "         [ 1.2247,  1.2247,  1.2247]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(27,dtype=torch.float32).reshape(3, 3, 3)\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[1]):\n",
    "        for k in range(x.shape[2]):\n",
    "            x[i][j][k] = i\n",
    "\n",
    "print(x)\n",
    "nn.LayerNorm((3,3,3))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "blocks = list()\n",
    "for layer in range(12):\n",
    "    block = dict(dict(dict(model.named_children())['blocks'].named_children())[str(layer)].named_children())\n",
    "    ls = [list(block['attn'].children())[0], list(block['attn'].children())[4],list(block['mlp'].children())[0], list(block['mlp'].children())[4]]\n",
    "\n",
    "    weight_ls = list()\n",
    "    for i in range(len(ls)):\n",
    "        weight_ls.append(ls[i].weight.clone().detach().T)\n",
    "        weight_ls.append(ls[i].bias.clone().detach())\n",
    "    blocks.append(weight_ls)\n",
    "\n",
    "    \n",
    "\n",
    "for i in blocks[0]:\n",
    "    print(i.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0200,  0.0100,  0.0200,  0.0200,  0.0700, -0.0100,  0.0700, -0.1100],\n",
       "        [ 0.0100,  0.0600, -0.0100, -0.1000,  0.0100,  0.0000,  0.0600, -0.0200],\n",
       "        [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0300,  0.0500, -0.0300,  0.0400,  0.0000,  0.0400, -0.0200,  0.0100],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0100, -0.0000, -0.0000,  0.0000],\n",
       "        [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(blocks[0][0][:8, 16:24] * 100)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1279)\n"
     ]
    }
   ],
   "source": [
    "blk1 = blocks[0]\n",
    "torch.manual_seed(10)\n",
    "blk1[0] = blk1[0].to(torch.float32)\n",
    "blk1[1] = blk1[1].to(torch.float32)\n",
    "\n",
    "dummy_input = torch.randn((4, 196, 768), dtype=torch.float32)\n",
    "print(torch.matmul(dummy_input,blk1[0])[0][0][1])\n",
    "dQKV = torch.add(torch.matmul(dummy_input,blk1[0]), blk1[1])\n",
    "dQKV = dQKV.view(4, 196, 12, 3, 64)\n",
    "dQKV = torch.einsum('b p h k d -> b h k p d',dQKV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 12, 3, 196, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dQKV.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 : batch\n",
    "12 : Multi head\n",
    "3 : QKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = dQKV[0][11][0]\n",
    "K = dQKV[0][11][1]\n",
    "V = dQKV[0][11][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1010,  0.0250,  0.1140, -0.0760,  0.2520,  0.0210,  0.1320,  0.0580],\n",
       "        [-0.1040, -0.0690,  0.0040,  0.0730,  0.1050, -0.0370,  0.0130,  0.0560],\n",
       "        [-0.1130, -0.0590, -0.1950, -0.0380,  0.0910, -0.0030,  0.1160, -0.0250],\n",
       "        [-0.1420, -0.0270,  0.0880,  0.0630,  0.0510, -0.0040,  0.1000, -0.0370],\n",
       "        [-0.0640,  0.0090,  0.2060,  0.0890,  0.1730,  0.1010,  0.0210, -0.0030],\n",
       "        [-0.1120, -0.0040,  0.0780,  0.0350,  0.2750,  0.0000, -0.0050,  0.0410],\n",
       "        [-0.0460,  0.1510,  0.0790, -0.0710,  0.0720, -0.1020,  0.1840,  0.2080],\n",
       "        [-0.2070,  0.0510,  0.0290,  0.0890,  0.1710,  0.0070, -0.0420,  0.0620]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.matmul(torch.nn.functional.softmax(torch.matmul(Q, K.T), dim=1),V)[188:196,8:16]#일반 ATTN\n",
    "torch.round(torch.matmul(torch.nn.functional.softmax(torch.matmul(Q, K.T), dim=1),V)[188:,64-8:64]*1000)/1000#일반 ATTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
       "        [0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
       "        [0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
       "        [0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
       "        [0.0117, 0.0317, 0.0861, 0.2341, 0.6364]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(torch.arange(25, dtype=torch.float).reshape(5,5),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0300, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.0000, 0.0000, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.0000, 0.0000, 0.1400, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.4800, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.matmul(Q, K.T)[:8,:8]\n",
    "tmp_emp = torch.exp(tmp - torch.max(tmp, dim=1)[0].view(-1, 1))\n",
    "torch.round(tmp_emp*100)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "++++++++++++++ 0 +++++++++++++++\n",
      "==P==\n",
      "0.0 1.0 0.0 0.0016 \n",
      "0.0 0.0335 0.0 1.0 \n",
      "0.0 1.0 0.0 0.0 \n",
      "0.0 1.0 0.0 0.0687 \n",
      "==L==\n",
      "1.0 1.03 1.0 1.07 1.0 1.14 1.48 1.0 \n",
      "==V==\n",
      "-1.0966 0.4385 0.467 0.3408 \n",
      "-0.3917 0.1036 -0.8208 0.2619 \n",
      "-1.5464 -0.1394 0.1607 0.5179 \n",
      "-1.4375 -1.5007 -2.1937 2.7009 \n",
      "-0.9083 -0.5875 -1.4028 0.1797 \n",
      "-0.5917 -0.4136 0.2884 -1.2574 \n",
      "-0.3567 0.127 -1.4185 1.1774 \n",
      "-0.6361 -0.8299 0.0542 0.0963 \n",
      "==O==\n",
      "-0.39 0.1 -0.82 0.27 \n",
      "-1.45 -1.5 -2.22 2.71 \n",
      "-0.39 0.1 -0.82 0.26 \n",
      "-0.49 0.0 -0.97 0.45 \n",
      "\n",
      "++++++++++++++ 1 +++++++++++++++\n",
      "==P==\n",
      "0.0028 0.0 0.0 0.0 \n",
      "0.0007 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0014 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.26 1.2 1.0 1.07 1.45 1.28 1.57 1.18 \n",
      "==V==\n",
      "-0.5827 -1.516 -0.9033 1.7767 \n",
      "-0.4209 -0.6328 -1.1421 -0.1415 \n",
      "-0.4936 0.6809 1.4148 0.539 \n",
      "-1.2153 0.7272 0.5814 -0.6602 \n",
      "0.0069 -1.2134 -2.8152 1.9007 \n",
      "-1.3064 -1.0124 -0.1074 -0.0034 \n",
      "-1.6961 0.5069 1.4507 0.0407 \n",
      "-0.7878 -1.4801 -1.2407 1.403 \n",
      "==O==\n",
      "-0.73 -0.16 -0.86 0.27 \n",
      "-1.47 -1.69 -2.64 2.99 \n",
      "-0.39 0.1 -0.82 0.26 \n",
      "-0.49 -0.0 -0.98 0.45 \n",
      "\n",
      "++++++++++++++ 2 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.3261 0.0108 0.0 \n",
      "0.0 0.0128 0.0013 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0187 0.0 0.0 \n",
      "==L==\n",
      "1.6 1.21 1.0 1.09 1.61 1.28 1.57 1.18 \n",
      "==V==\n",
      "-1.3774 -0.1605 -0.2836 0.3993 \n",
      "-0.3503 -0.2889 -0.6377 -0.5739 \n",
      "-0.6172 -1.1759 -0.9624 1.9045 \n",
      "-1.3225 -0.9965 -0.8552 0.5977 \n",
      "-1.2184 0.5879 -0.6668 0.0443 \n",
      "-0.7973 -1.0769 -0.7404 -1.1029 \n",
      "-1.5381 -0.5156 -1.015 1.721 \n",
      "-0.2614 -1.2228 -1.4586 -0.3207 \n",
      "==O==\n",
      "-0.85 -0.27 -1.08 0.11 \n",
      "-1.48 -1.7 -2.65 2.98 \n",
      "-0.39 0.1 -0.82 0.26 \n",
      "-0.5 -0.01 -0.99 0.44 \n",
      "\n",
      "++++++++++++++ 3 +++++++++++++++\n",
      "==P==\n",
      "1.0 0.0 0.0 0.0535 \n",
      "0.0025 0.0 0.0 0.0072 \n",
      "0.0001 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0118 \n",
      "==L==\n",
      "1.78 1.22 1.0 1.1 2.59 1.29 1.58 1.19 \n",
      "==V==\n",
      "-0.4517 -0.6187 -0.4007 -0.1045 \n",
      "-0.6495 -1.0439 -1.3781 1.4385 \n",
      "-1.2837 -0.037 0.3531 -0.6871 \n",
      "-1.4308 -1.7049 -0.8589 1.0144 \n",
      "-1.5612 0.7705 0.9905 -0.3273 \n",
      "-1.4599 0.4847 0.3363 1.2009 \n",
      "-1.0862 -1.5305 0.7886 -0.9128 \n",
      "-1.565 0.624 -0.2963 0.1073 \n",
      "==O==\n",
      "-0.91 -0.83 -0.93 -0.0 \n",
      "-1.49 -1.71 -2.65 2.99 \n",
      "-0.39 0.1 -0.82 0.26 \n",
      "-0.51 -0.03 -1.0 0.46 \n",
      "\n",
      "++++++++++++++ 4 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0011 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0001 0.0 0.0 \n",
      "==L==\n",
      "1.78 1.26 1.0 1.1 3.16 1.59 1.58 1.04 \n",
      "==V==\n",
      "-1.0042 -0.6537 -0.287 0.5103 \n",
      "-0.5677 -1.2194 -1.3352 0.3194 \n",
      "-1.1201 -0.1654 0.4767 0.2985 \n",
      "-0.9787 0.4823 -0.0191 0.0788 \n",
      "-0.146 0.0143 -2.1035 0.9965 \n",
      "-1.3504 -1.4903 0.1811 0.9921 \n",
      "-1.294 -0.6133 -0.3566 -0.4063 \n",
      "-1.2346 -0.7464 0.4603 0.2691 \n",
      "==O==\n",
      "-0.91 -0.83 -0.93 -0.0 \n",
      "-1.54 -1.74 -2.64 3.0 \n",
      "-0.39 0.1 -0.82 0.26 \n",
      "-0.51 -0.03 -1.0 0.46 \n",
      "\n",
      "++++++++++++++ 5 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.031 \n",
      "0.0 0.0 0.0 0.0076 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0309 \n",
      "==L==\n",
      "1.81 1.27 1.0 1.13 3.16 1.59 1.59 1.04 \n",
      "==V==\n",
      "-1.0891 0.4025 0.2366 -0.9368 \n",
      "-1.6846 -2.1611 -0.5529 1.0184 \n",
      "-0.7201 -0.7835 0.2875 -0.0828 \n",
      "-0.1927 -1.7602 -0.2014 0.9888 \n",
      "-0.8577 -0.1943 -1.1682 1.0298 \n",
      "-0.3419 -0.5692 -0.4251 0.7711 \n",
      "-0.8732 -0.1871 -0.8964 0.7203 \n",
      "-0.5817 0.1048 -2.1424 0.0403 \n",
      "==O==\n",
      "-0.92 -0.89 -0.94 0.03 \n",
      "-1.54 -1.76 -2.64 3.01 \n",
      "-0.39 0.1 -0.82 0.26 \n",
      "-0.52 -0.08 -1.01 0.49 \n",
      "\n",
      "++++++++++++++ 6 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0151 0.0 0.0 \n",
      "0.0 1.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0884 0.0296 0.0 \n",
      "==L==\n",
      "1.83 1.07 1.0 1.25 1.1 1.61 1.47 1.09 \n",
      "==V==\n",
      "-0.73 -1.0049 -2.0123 1.5072 \n",
      "-1.0191 -2.0097 -1.8504 1.5417 \n",
      "-0.3815 0.3653 -1.3466 0.6093 \n",
      "-0.8021 -0.5342 -1.1482 0.2166 \n",
      "-0.8077 0.1656 0.1702 0.464 \n",
      "-0.3791 -0.6327 -1.0643 0.0513 \n",
      "-1.1178 -0.2091 0.9922 0.1554 \n",
      "-0.3497 -0.5118 0.0287 0.8821 \n",
      "==O==\n",
      "-0.94 -0.92 -0.97 0.05 \n",
      "-1.1 -2.11 -2.0 1.71 \n",
      "-0.39 0.1 -0.82 0.26 \n",
      "-0.62 -0.25 -1.21 0.64 \n",
      "\n",
      "++++++++++++++ 7 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 1.0 0.0 \n",
      "0.0 0.0 1.0 0.0 \n",
      "0.0 0.0 0.0043 0.0 \n",
      "0.0 0.0 1.0 0.0 \n",
      "==L==\n",
      "1.02 1.08 1.0 1.01 1.0 1.74 1.01 1.1 \n",
      "==V==\n",
      "-1.151 0.1206 -1.1827 1.1885 \n",
      "-0.6193 -0.7195 0.0182 -0.7938 \n",
      "-1.1315 -1.0661 0.6363 0.9106 \n",
      "-1.4694 -0.8866 -1.3997 1.5678 \n",
      "-1.719 -0.6366 0.2033 2.2318 \n",
      "-1.079 -1.3058 -1.9506 1.2037 \n",
      "-1.2756 -1.1856 -0.7013 -0.4745 \n",
      "-1.1623 0.2983 0.5711 -0.2047 \n",
      "==O==\n",
      "-1.14 -1.08 0.63 0.91 \n",
      "-1.21 -1.22 0.49 1.04 \n",
      "-0.4 0.1 -0.82 0.27 \n",
      "-1.14 -1.07 0.62 0.92 \n",
      "\n",
      "++++++++++++++ 8 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.02 1.08 1.0 1.01 1.0 1.74 1.01 1.1 \n",
      "==V==\n",
      "-0.2303 -0.4631 0.2337 1.1718 \n",
      "-0.9275 -0.1431 -2.1973 1.6135 \n",
      "-0.9397 -0.1518 -0.5455 1.469 \n",
      "-1.2613 -0.0314 1.8288 -0.2424 \n",
      "-1.7696 -0.5492 -0.144 1.1107 \n",
      "-1.8832 -1.7404 -0.1081 0.0183 \n",
      "0.1672 -0.513 -1.1832 -0.4008 \n",
      "-0.3749 -0.2476 -0.98 1.3302 \n",
      "==O==\n",
      "-1.14 -1.08 0.63 0.91 \n",
      "-1.21 -1.22 0.49 1.04 \n",
      "-0.4 0.1 -0.82 0.27 \n",
      "-1.14 -1.07 0.62 0.92 \n",
      "\n",
      "++++++++++++++ 9 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0002 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.02 1.08 1.01 1.01 1.0 1.77 1.01 1.1 \n",
      "==V==\n",
      "-0.6019 -0.2954 -1.2123 0.1863 \n",
      "-0.3948 -1.0612 -0.1517 0.1456 \n",
      "-1.211 -0.458 0.9946 -0.4854 \n",
      "-0.412 -0.6854 -2.0723 1.1474 \n",
      "-0.822 -0.5882 -2.5142 2.5166 \n",
      "-1.7737 -0.7125 -1.0426 0.8651 \n",
      "-0.6299 -0.819 -1.2635 0.0746 \n",
      "-0.6717 -0.3449 -0.5792 0.1544 \n",
      "==O==\n",
      "-1.14 -1.08 0.63 0.91 \n",
      "-1.21 -1.22 0.49 1.04 \n",
      "-0.4 0.1 -0.82 0.27 \n",
      "-1.14 -1.07 0.62 0.92 \n",
      "\n",
      "++++++++++++++ 10 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0003 \n",
      "0.0 0.0 0.0 0.0006 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.02 1.08 1.01 1.01 1.0 2.03 1.01 1.42 \n",
      "==V==\n",
      "-0.9123 -1.1715 -1.5245 0.9122 \n",
      "-0.0015 -1.4588 -1.2373 -0.6834 \n",
      "-0.3619 -0.348 -0.4269 0.3324 \n",
      "-1.7057 -1.4064 0.0539 1.5507 \n",
      "-0.5386 -0.7263 -0.6217 0.731 \n",
      "-0.8543 -1.0917 -1.2695 0.8847 \n",
      "0.1555 -0.0281 -0.0721 0.6148 \n",
      "-1.1917 -1.3898 -1.7106 0.3667 \n",
      "==O==\n",
      "-1.14 -1.08 0.63 0.91 \n",
      "-1.22 -1.23 0.48 1.04 \n",
      "-0.4 0.1 -0.82 0.27 \n",
      "-1.14 -1.07 0.62 0.92 \n",
      "\n",
      "++++++++++++++ 11 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0004 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.02 1.08 1.01 1.01 1.0 2.04 1.02 1.42 \n",
      "==V==\n",
      "-1.3679 0.376 -0.9071 0.6562 \n",
      "-1.4542 -0.331 0.6485 0.7435 \n",
      "-0.1906 0.5576 -0.7537 -0.4331 \n",
      "0.4201 0.6576 -1.0823 -0.6445 \n",
      "-0.6954 0.634 0.9564 -1.0602 \n",
      "-0.7469 0.0696 -0.2692 -0.0679 \n",
      "-1.016 -0.8303 0.4633 -0.4184 \n",
      "-0.0053 0.3097 -0.1235 -0.8421 \n",
      "==O==\n",
      "-1.14 -1.08 0.63 0.91 \n",
      "-1.22 -1.23 0.48 1.04 \n",
      "-0.4 0.1 -0.82 0.27 \n",
      "-1.14 -1.07 0.62 0.92 \n",
      "\n",
      "++++++++++++++ 12 +++++++++++++++\n",
      "==P==\n",
      "0.0006 0.0 0.0 0.0 \n",
      "0.0006 0.0 0.0 0.0 \n",
      "0.0369 0.0 0.0 0.0 \n",
      "0.0001 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.02 1.08 1.04 1.01 1.0 2.47 1.02 2.38 \n",
      "==V==\n",
      "-0.9435 -0.042 -0.5732 -0.3733 \n",
      "-0.2588 -0.255 -2.411 -0.3176 \n",
      "-0.1969 0.29 -0.0831 -1.3238 \n",
      "-0.6836 -0.8322 0.7761 0.2426 \n",
      "-0.9365 -2.0899 -1.1577 2.6908 \n",
      "-0.1803 -0.6834 -1.685 0.7364 \n",
      "-1.2586 -0.7605 -0.4934 -0.4654 \n",
      "-0.6765 -0.3541 0.0863 0.3109 \n",
      "==O==\n",
      "-1.14 -1.08 0.63 0.91 \n",
      "-1.22 -1.23 0.48 1.04 \n",
      "-0.43 0.1 -0.84 0.25 \n",
      "-1.14 -1.07 0.62 0.92 \n",
      "\n",
      "++++++++++++++ 13 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.02 1.08 1.04 1.01 1.0 2.47 1.02 2.38 \n",
      "==V==\n",
      "-0.887 0.1023 -0.7043 1.1797 \n",
      "-1.4574 0.0387 0.0339 -0.2001 \n",
      "-0.6456 -0.1081 -0.2283 1.4746 \n",
      "-1.7738 -1.0161 -0.5035 -0.7225 \n",
      "-0.4146 -1.0203 -2.0327 -0.5744 \n",
      "-1.3365 -0.0151 -0.3696 1.169 \n",
      "-0.8367 -1.8021 -0.8005 -0.3946 \n",
      "-0.6761 0.7209 -1.4546 -0.4522 \n",
      "==O==\n",
      "-1.14 -1.08 0.63 0.91 \n",
      "-1.22 -1.23 0.48 1.04 \n",
      "-0.43 0.1 -0.84 0.25 \n",
      "-1.14 -1.07 0.62 0.92 \n",
      "\n",
      "++++++++++++++ 14 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.02 1.08 1.04 1.01 1.0 2.47 1.02 2.38 \n",
      "==V==\n",
      "-0.4542 0.7759 -0.5213 0.0137 \n",
      "-0.548 -0.5222 -0.8902 0.124 \n",
      "-0.6846 -0.216 -0.9446 0.2479 \n",
      "-0.7716 -0.4736 0.3871 1.2677 \n",
      "-0.4214 0.7501 -0.1623 -1.6993 \n",
      "-1.0013 0.0912 -0.3821 0.9449 \n",
      "-1.5654 -1.4869 -1.0218 -0.059 \n",
      "-0.3034 -0.2691 0.3306 0.6947 \n",
      "==O==\n",
      "-1.14 -1.08 0.63 0.91 \n",
      "-1.22 -1.23 0.48 1.04 \n",
      "-0.43 0.1 -0.84 0.25 \n",
      "-1.14 -1.07 0.62 0.92 \n",
      "\n",
      "++++++++++++++ 15 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.4826 0.0002 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0001 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.02 1.57 1.04 1.02 1.0 1.41 1.07 1.0 \n",
      "==V==\n",
      "-1.0322 -1.9412 -0.1548 0.5434 \n",
      "-0.8284 -1.147 -0.197 0.8823 \n",
      "-0.9769 -0.1026 0.6885 -0.3375 \n",
      "-0.6875 -0.2914 -1.0476 -0.3289 \n",
      "-0.8692 -1.3396 -1.2912 0.916 \n",
      "-0.5209 -0.7073 -0.4231 -0.6106 \n",
      "-0.8343 -1.2493 -1.5328 1.4166 \n",
      "-0.8542 -1.6997 -1.2772 1.5015 \n",
      "==O==\n",
      "-1.14 -1.08 0.63 0.91 \n",
      "-1.72 -2.16 0.41 1.3 \n",
      "-0.43 0.1 -0.84 0.25 \n",
      "-1.14 -1.07 0.62 0.92 \n",
      "\n",
      "++++++++++++++ 16 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 1.0 \n",
      "0.0 0.0 0.0 1.0 \n",
      "0.0 0.0 0.0 1.0 \n",
      "0.0 0.0 0.0 1.0 \n",
      "==L==\n",
      "1.03 1.45 1.45 1.01 1.01 2.29 1.1 1.0 \n",
      "==V==\n",
      "-0.0725 -0.2996 -0.3587 -0.6645 \n",
      "-0.7321 -0.1588 -0.7602 -0.1584 \n",
      "-1.3474 -2.2677 -1.2264 1.6575 \n",
      "-1.4948 -0.8216 -0.2444 1.1016 \n",
      "-0.7883 -0.8357 0.9419 -0.2398 \n",
      "-0.7176 1.4001 0.1274 -0.015 \n",
      "-1.615 -0.5709 -1.662 1.005 \n",
      "-0.9534 -0.9374 -1.1196 1.2953 \n",
      "==O==\n",
      "-1.53 -0.86 -0.22 1.13 \n",
      "-1.99 -1.45 -0.13 1.48 \n",
      "-1.68 -0.78 -0.6 1.21 \n",
      "-1.5 -0.83 -0.24 1.11 \n",
      "\n",
      "++++++++++++++ 17 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.03 1.45 1.45 1.01 1.01 2.29 1.1 1.0 \n",
      "==V==\n",
      "-0.682 0.1098 -0.5964 0.1464 \n",
      "-0.9506 -0.1641 -0.9952 0.0361 \n",
      "-0.4263 -0.8412 -0.035 0.0512 \n",
      "-1.205 -0.578 -0.4337 0.9124 \n",
      "-0.5479 -0.0229 0.3036 -1.0801 \n",
      "-0.6062 0.286 0.2106 0.0258 \n",
      "-0.3393 -0.6399 -1.0764 0.5914 \n",
      "-0.9364 0.5202 0.6379 -0.9971 \n",
      "==O==\n",
      "-1.53 -0.86 -0.22 1.13 \n",
      "-1.99 -1.45 -0.13 1.48 \n",
      "-1.68 -0.78 -0.6 1.21 \n",
      "-1.5 -0.83 -0.24 1.11 \n",
      "\n",
      "++++++++++++++ 18 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.03 1.45 1.45 1.01 1.01 2.29 1.1 1.0 \n",
      "==V==\n",
      "-0.6024 0.1996 -0.7786 -0.4699 \n",
      "-0.9224 -0.9584 -0.1638 0.2713 \n",
      "-1.3271 -0.0793 -1.0663 -0.2399 \n",
      "-1.62 -0.986 -0.5061 0.5315 \n",
      "-0.3988 -0.8358 -0.5058 0.4592 \n",
      "-1.5098 -1.7953 -2.4123 2.3224 \n",
      "-1.3927 -0.8341 -0.2874 1.8359 \n",
      "-1.1371 0.0393 -0.5911 -1.112 \n",
      "==O==\n",
      "-1.53 -0.86 -0.22 1.13 \n",
      "-1.99 -1.45 -0.13 1.48 \n",
      "-1.68 -0.78 -0.6 1.21 \n",
      "-1.5 -0.83 -0.24 1.11 \n",
      "\n",
      "++++++++++++++ 19 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.03 1.45 1.45 1.01 1.01 2.29 1.1 1.0 \n",
      "==V==\n",
      "-0.3262 -1.7009 -1.2777 1.8212 \n",
      "-1.7943 0.0256 0.257 0.0701 \n",
      "-1.2092 0.577 -0.2989 -0.6636 \n",
      "-1.5289 -0.5984 -1.0815 0.2812 \n",
      "-1.8765 0.1306 1.3385 0.77 \n",
      "-1.057 -0.061 0.5258 -0.0728 \n",
      "-1.0554 0.2923 -1.4761 0.1564 \n",
      "-0.9619 0.5692 2.2591 -0.9201 \n",
      "==O==\n",
      "-1.53 -0.86 -0.22 1.13 \n",
      "-1.99 -1.45 -0.13 1.48 \n",
      "-1.68 -0.78 -0.6 1.21 \n",
      "-1.5 -0.83 -0.24 1.11 \n",
      "\n",
      "++++++++++++++ 20 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.03 1.46 1.45 1.03 1.01 2.29 1.11 1.0 \n",
      "==V==\n",
      "-1.441 0.3313 0.6701 -1.1654 \n",
      "-1.0091 0.1998 -0.3136 -0.9133 \n",
      "-0.923 -0.4383 -0.026 1.4423 \n",
      "-1.3526 -0.0075 -0.4067 0.7422 \n",
      "-1.2746 -2.1533 -1.9491 0.2796 \n",
      "-0.0298 -0.2398 -0.8152 -0.9127 \n",
      "-0.7723 -0.0332 -0.5433 0.0921 \n",
      "-1.1564 0.3221 -0.0264 -0.7547 \n",
      "==O==\n",
      "-1.53 -0.86 -0.22 1.13 \n",
      "-1.99 -1.45 -0.13 1.48 \n",
      "-1.68 -0.78 -0.6 1.21 \n",
      "-1.53 -0.87 -0.28 1.11 \n",
      "\n",
      "++++++++++++++ 21 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0001 0.0 0.0 \n",
      "==L==\n",
      "1.03 1.46 1.45 1.03 1.01 2.29 1.11 1.0 \n",
      "==V==\n",
      "-1.0667 -0.1416 0.3373 -0.1042 \n",
      "-0.6986 -0.4492 -1.0887 1.9378 \n",
      "-0.9634 -0.6677 -0.6863 -0.3906 \n",
      "-0.6568 -0.2789 0.5775 -0.6549 \n",
      "-1.1861 -1.16 -1.7876 1.4662 \n",
      "-0.421 -1.2611 -1.636 0.5054 \n",
      "-0.8698 0.6496 0.0936 0.1478 \n",
      "-1.4648 0.5142 1.0591 0.1872 \n",
      "==O==\n",
      "-1.53 -0.86 -0.22 1.13 \n",
      "-1.99 -1.45 -0.13 1.48 \n",
      "-1.68 -0.78 -0.6 1.21 \n",
      "-1.53 -0.87 -0.28 1.11 \n",
      "\n",
      "++++++++++++++ 22 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0001 \n",
      "0.0 0.0 0.0 0.1426 \n",
      "0.0 0.0 0.0 0.0001 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.03 1.6 1.45 1.03 1.02 2.3 1.12 1.0 \n",
      "==V==\n",
      "-0.1157 -0.994 -1.787 2.0837 \n",
      "-1.0194 -0.2027 0.4567 -0.7715 \n",
      "-0.801 -1.3665 -0.6597 0.1882 \n",
      "-1.7074 -1.394 -0.2733 1.3735 \n",
      "-1.7562 0.2769 -1.5283 1.5424 \n",
      "-1.0185 -1.3246 -1.1957 1.0044 \n",
      "-0.4057 0.3627 -0.5775 0.1059 \n",
      "-0.8153 -0.1413 -1.4685 0.1672 \n",
      "==O==\n",
      "-1.53 -0.86 -0.22 1.13 \n",
      "-2.24 -1.65 -0.17 1.68 \n",
      "-1.68 -0.78 -0.6 1.21 \n",
      "-1.53 -0.87 -0.28 1.11 \n",
      "\n",
      "++++++++++++++ 23 +++++++++++++++\n",
      "==P==\n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 \n",
      "==L==\n",
      "1.03 1.6 1.45 1.03 1.02 2.3 1.12 1.0 \n",
      "==V==\n",
      "-0.3131 -0.7819 0.7328 0.0255 \n",
      "-1.3352 -0.4977 -0.864 -0.8782 \n",
      "-1.2503 -1.3025 0.3865 -0.2254 \n",
      "-0.7815 -1.7921 -0.0578 0.7324 \n",
      "-1.1586 -0.8496 -0.048 0.7054 \n",
      "-0.7576 -0.6462 -0.621 0.1232 \n",
      "-0.8635 -0.9996 -0.6706 1.2907 \n",
      "-0.7994 -1.2262 -1.0716 0.512 \n",
      "==O==\n",
      "-1.53 -0.86 -0.22 1.13 \n",
      "-2.24 -1.65 -0.17 1.68 \n",
      "-1.68 -0.78 -0.6 1.21 \n",
      "-1.53 -0.87 -0.28 1.11 \n",
      "-1.482\t-0.829\t-0.216\t1.095\t\n",
      "-1.4\t-1.033\t-0.106\t1.048\t\n",
      "-1.161\t-0.539\t-0.418\t0.836\t\n",
      "-1.488\t-0.849\t-0.271\t1.084\t\n"
     ]
    }
   ],
   "source": [
    "Q = dQKV[0][0][0]\n",
    "K = dQKV[0][0][1]\n",
    "\n",
    "V = dQKV[0][0][2]\n",
    "M = torch.Tensor(8*[-10000000]).view(-1, 1)\n",
    "L = torch.zeros(8).view(-1,1)\n",
    "O = torch.zeros((8,64))\n",
    "\n",
    "for i in range(0, 192, 8):\n",
    "\n",
    "    tmp = torch.matmul(Q[:8,:], K[i:i+8,:].T)\n",
    "    tmp_max = torch.maximum(torch.max(tmp, dim=1)[0].view(-1, 1), M)\n",
    "    P = torch.exp(tmp - tmp_max)\n",
    "    # print(\"==K==\")\n",
    "    # for j in K[i:i+8,:4]:\n",
    "    #     for k in j:\n",
    "    #         print(round(k.item(),4),end=\" \")\n",
    "    #     print()\n",
    "    print('\\n++++++++++++++',i//8,'+++++++++++++++')\n",
    "    print(\"==P==\")\n",
    "    for j in P[:4,:4]:\n",
    "        for k in j:\n",
    "            print(round(k.item(),4),end=\" \")\n",
    "        print()\n",
    "    \n",
    "    L = L * torch.exp(M-tmp_max) + torch.sum(P, dim=1).view(-1, 1)\n",
    "    print(\"==L==\")\n",
    "    for j in L.flatten():\n",
    "        print(round(j.item(), 2), end=\" \")\n",
    "    print()\n",
    "    if M[0][0] > -1000:\n",
    "        O = O*torch.exp(M-tmp_max) + torch.matmul(P, V[i:i+8,:])\n",
    "    else:\n",
    "        O = torch.matmul(P, V[i:i+8,:])\n",
    "\n",
    "    print(\"==V==\")\n",
    "    for j in V[i:i+8,:4]:\n",
    "        for k in j:\n",
    "            print(round(k.item(),4),end=\" \")\n",
    "        print()\n",
    "    print(\"==O==\")\n",
    "    for j in O[:4,:4]:\n",
    "        for k in j:\n",
    "            print(round(k.item(),2),end=\" \")\n",
    "        print()\n",
    "    M = tmp_max\n",
    "\n",
    "O = O/L\n",
    "\n",
    "torch.round(O[:4,:4])\n",
    "for i in O[:4,:4]:\n",
    "    for j in i:\n",
    "        print(round(j.item(),3),end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[58.0175],\n",
      "        [41.0226],\n",
      "        [49.0482],\n",
      "        [51.3955],\n",
      "        [50.6254],\n",
      "        [45.4523],\n",
      "        [49.4336],\n",
      "        [58.0317]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[58.0175],\n",
       "        [41.0226],\n",
       "        [49.0482],\n",
       "        [51.3955],\n",
       "        [50.6254],\n",
       "        [47.0000],\n",
       "        [55.0000],\n",
       "        [63.0000]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.sum(torch.Tensor(7*[8*[-100000000]]), dim=1)\n",
    "print(M)\n",
    "torch.maximum(torch.max(torch.arange(64).reshape(8,8), dim=1)[0].view(-1, 1), M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([4, 12, 3, 196, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0171, -0.1672,  0.1841, -1.3738, -1.8454,  0.5362, -1.0144,  0.1519],\n",
       "        [ 0.8407, -1.9227,  1.2822,  0.7117, -1.0879,  1.1466, -0.5234,  0.6682],\n",
       "        [ 2.8590, -1.6855,  0.8893,  0.6758, -0.0857,  1.0527, -0.9685, -0.3995],\n",
       "        [-0.9083, -0.5875, -1.4028,  0.1797,  0.8012, -0.4761,  1.7169, -0.2354],\n",
       "        [-1.0891,  0.4025,  0.2366, -0.9368,  0.4184,  0.6489,  2.2066,  0.3225],\n",
       "        [-0.8220, -0.5882, -2.5142,  2.5166,  1.8524, -2.0508,  1.9896, -1.5995],\n",
       "        [-0.4542,  0.7759, -0.5213,  0.0137,  0.5711, -0.8187,  1.1955,  1.3803],\n",
       "        [-0.3988, -0.8358, -0.5058,  0.4592, -0.0558, -1.9046,  1.9274, -0.3382]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dQKV.reshape(4, 196, 2304)[0][8:16,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kkh38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
